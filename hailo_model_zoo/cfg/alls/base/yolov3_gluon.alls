normalization1 = normalization([123.675, 116.28, 103.53], [58.395, 57.12, 57.375])
model_optimization_config(calibration, batch_size=2, calibset_size=64)
model_optimization_flavor(compression_level=2)
pre_quantization_optimization(ew_add_fusing, policy=enabled, infusible_ew_add_type=conv)
post_quantization_optimization(finetune, policy=enabled, learning_rate=0.0001, epochs=8, dataset_size=4000)
quantization_param(output_layer1, precision_mode=a16_w16)
quantization_param(output_layer2, precision_mode=a16_w16)
quantization_param(output_layer3, precision_mode=a16_w16)
