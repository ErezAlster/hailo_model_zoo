model_optimization_flavor(optimization_level=0)
pre_quantization_optimization(matmul_equalization, layers=[matmul1,  matmul3, matmul5, matmul7, matmul9], policy=enabled, matmul_bias=enabled)
pre_quantization_optimization(equalization, layers=[ew_add1], equalization_target=activation, policy=enabled)

pre_quantization_optimization(matmul_correction, layers={matmul*}, correction_type=zp_comp_block_3)
model_optimization_config(globals, output_16bit=enabled)
quantization_param(output_layer1, precision_mode=a16_w16)
quantization_param(output_layer2, precision_mode=a16_w16)
quantization_param({ew_sub_softmax*}, activation_fit=enabled)
quantization_param(ew_sub_softmax1, precision_mode=a16_w16)
quantization_param(reduce_sum_softmax1, precision_mode=a16_w16)
quantization_param(ew_sub_softmax3, precision_mode=a16_w16)
quantization_param(reduce_sum_softmax3, precision_mode=a16_w16)
quantization_param(ew_sub_softmax5, precision_mode=a16_w16)
quantization_param(reduce_sum_softmax5, precision_mode=a16_w16)
quantization_param(concat1, precision_mode=a16_w16)
quantization_param(conv41, precision_mode=a16_w16)

performance_param(compiler_optimization_level=1, optimize_for_batch=1)

{} = defuse_block(matmul1, matmul2, precision_change0, precision_change1, precision_change2, precision_change3, ew_mult_softmax1, ew_sub_softmax1, reduce_max_softmax1, reduce_sum_softmax1, defuse_count=4)
{} = defuse_block(matmul5, matmul6, precision_change4, precision_change5, precision_change6, precision_change7, ew_mult_softmax3, ew_sub_softmax3, reduce_max_softmax3, reduce_sum_softmax3, defuse_count=4)
{} = defuse_block(matmul9, matmul10, precision_change8, precision_change9, precision_change10, precision_change11, ew_mult_softmax5, ew_sub_softmax5, reduce_max_softmax5, reduce_sum_softmax5, defuse_count=4)
