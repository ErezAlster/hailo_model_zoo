base:
- base/coco_single_person.yaml
preprocessing:
  network_type: single_person_pose_estimation
  meta_arch: vit_pose
postprocessing:
  network_type: vit_pose
network:
  network_name: vit_pose_small_bn
paths:
  alls_script: vit_pose_small_bn.alls
  network_path:
  - models_files/SinglePersonPoseEstimation/vit/vit_pose_small_bn/pretrained/2023-07-20/vit_pose_small_bn.onnx
  url: https://hailo-model-zoo.s3.eu-west-2.amazonaws.com/SinglePersonPoseEstimation/vit/vit_pose_small_bn/pretrained/2023-07-20/vit_pose_small_bn.zip
parser:
  nodes:
  - x.1
  - - '1095'
  normalization_params:
    normalize_in_net: true
    mean_list:
    - 123.675
    - 116.28
    - 103.53
    std_list:
    - 58.395
    - 57.12
    - 57.375
evaluation:
  network_type: single_person_pose_estimation
  infer_type: np_infer
info:
  task: single person pose estimation
  input_shape: 256x192x3
  output_shape: 64x48x17
  operations: 10.7G
  parameters: 24.32M
  framework: pytorch
  eval_metric: AP
  full_precision_result: 72.01
  source: https://github.com/ViTAE-Transformer/ViTPose
  license_url: https://github.com/ViTAE-Transformer/ViTPose/blob/main/LICENSE
